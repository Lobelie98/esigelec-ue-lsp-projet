<extensionManifest><systemApiVersion>1.10.0</systemApiVersion><extensions><extension><name>org.apache.nifi.processors.hive.SelectHiveQL</name><type>PROCESSOR</type><deprecationNotice/><description>Execute provided HiveQL SELECT query against a Hive database connection. Query result will be converted to Avro or CSV format. Streaming is used so arbitrarily large result sets are supported. This processor can be scheduled to run on a timer, or cron expression, using the standard scheduling methods, or it can be triggered by an incoming FlowFile. If it is triggered by an incoming FlowFile, then attributes of that FlowFile will be available when evaluating the select query. FlowFile attribute 'selecthiveql.row.count' indicates how many rows were selected.</description><tags><tag>hive</tag><tag>sql</tag><tag>select</tag><tag>jdbc</tag><tag>query</tag><tag>database</tag></tags><properties><property><name>Hive Database Connection Pooling Service</name><displayName>Hive Database Connection Pooling Service</displayName><description>The Hive Controller Service that is used to obtain connection(s) to the Hive database</description><defaultValue></defaultValue><controllerServiceDefinition><className>org.apache.nifi.dbcp.hive.HiveDBCPService</className><groupId>org.apache.nifi</groupId><artifactId>nifi-hive-services-api-nar</artifactId><version>1.10.0</version></controllerServiceDefinition><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-pre-query</name><displayName>HiveQL Pre-Query</displayName><description>A semicolon-delimited list of queries executed before the main SQL query is executed. Example: 'set tez.queue.name=queue1; set hive.exec.orc.split.strategy=ETL; set hive.exec.reducers.bytes.per.reducer=1073741824'. Note, the results/outputs of these queries will be suppressed if successfully executed.</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-query</name><displayName>HiveQL Select Query</displayName><description>HiveQL SELECT query to execute. If this is not set, the query is assumed to be in the content of an incoming FlowFile.</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-post-query</name><displayName>HiveQL Post-Query</displayName><description>A semicolon-delimited list of queries executed after the main SQL query is executed. Note, the results/outputs of these queries will be suppressed if successfully executed.</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-fetch-size</name><displayName>Fetch Size</displayName><description>The number of result rows to be fetched from the result set at a time. This is a hint to the driver and may not be honored and/or exact. If the value specified is zero, then the hint is ignored.</description><defaultValue>0</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-max-rows</name><displayName>Max Rows Per Flow File</displayName><description>The maximum number of result rows that will be included in a single FlowFile. This will allow you to break up very large result sets into multiple FlowFiles. If the value specified is zero, then all rows are returned in a single FlowFile.</description><defaultValue>0</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-max-frags</name><displayName>Maximum Number of Fragments</displayName><description>The maximum number of fragments. If the value specified is zero, then all fragments are returned. This prevents OutOfMemoryError when this processor ingests huge table.</description><defaultValue>0</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-output-format</name><displayName>Output Format</displayName><description>How to represent the records coming from Hive (Avro, CSV, e.g.)</description><defaultValue>Avro</defaultValue><allowableValues><allowableValue><displayName>Avro</displayName><value>Avro</value><description></description></allowableValue><allowableValue><displayName>CSV</displayName><value>CSV</value><description></description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-normalize-avro</name><displayName>Normalize Table/Column Names</displayName><description>Whether to change non-Avro-compatible characters in column names to Avro-compatible characters. For example, colons and periods will be changed to underscores in order to build a valid Avro record.</description><defaultValue>false</defaultValue><allowableValues><allowableValue><displayName>true</displayName><value>true</value><description></description></allowableValue><allowableValue><displayName>false</displayName><value>false</value><description></description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>csv-header</name><displayName>CSV Header</displayName><description>Include Header in Output</description><defaultValue>true</defaultValue><allowableValues><allowableValue><displayName>true</displayName><value>true</value><description></description></allowableValue><allowableValue><displayName>false</displayName><value>false</value><description></description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>csv-alt-header</name><displayName>Alternate CSV Header</displayName><description>Comma separated list of header fields</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>csv-delimiter</name><displayName>CSV Delimiter</displayName><description>CSV Delimiter used to separate fields</description><defaultValue>,</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>csv-quote</name><displayName>CSV Quote</displayName><description>Whether to force quoting of CSV fields. Note that this might conflict with the setting for CSV Escape.</description><defaultValue>true</defaultValue><allowableValues><allowableValue><displayName>true</displayName><value>true</value><description></description></allowableValue><allowableValue><displayName>false</displayName><value>false</value><description></description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>csv-escape</name><displayName>CSV Escape</displayName><description>Whether to escape CSV strings in output. Note that this might conflict with the setting for CSV Quote.</description><defaultValue>true</defaultValue><allowableValues><allowableValue><displayName>true</displayName><value>true</value><description></description></allowableValue><allowableValue><displayName>false</displayName><value>false</value><description></description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-charset</name><displayName>Character Set</displayName><description>Specifies the character set of the record data.</description><defaultValue>UTF-8</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property></properties><dynamicProperties></dynamicProperties><relationships><relationship><name>success</name><description>Successfully created FlowFile from HiveQL query result set.</description><autoTerminated>false</autoTerminated></relationship><relationship><name>failure</name><description>HiveQL query execution failed. Incoming FlowFile will be penalized and routed to this relationship.</description><autoTerminated>false</autoTerminated></relationship></relationships><dynamicRelationship></dynamicRelationship><readsAttributes></readsAttributes><writesAttributes><writesAttribute><name>mime.type</name><description>Sets the MIME type for the outgoing flowfile to application/avro-binary for Avro or text/csv for CSV.</description></writesAttribute><writesAttribute><name>filename</name><description>Adds .avro or .csv to the filename attribute depending on which output format is selected.</description></writesAttribute><writesAttribute><name>selecthiveql.row.count</name><description>Indicates how many rows were selected/returned by the query.</description></writesAttribute><writesAttribute><name>fragment.identifier</name><description>If 'Max Rows Per Flow File' is set then all FlowFiles from the same query result set will have the same value for the fragment.identifier attribute. This can then be used to correlate the results.</description></writesAttribute><writesAttribute><name>fragment.count</name><description>If 'Max Rows Per Flow File' is set then this is the total number of  FlowFiles produced by a single ResultSet. This can be used in conjunction with the fragment.identifier attribute in order to know how many FlowFiles belonged to the same incoming ResultSet.</description></writesAttribute><writesAttribute><name>fragment.index</name><description>If 'Max Rows Per Flow File' is set then the position of this FlowFile in the list of outgoing FlowFiles that were all derived from the same result set FlowFile. This can be used in conjunction with the fragment.identifier attribute to know which FlowFiles originated from the same query result set and in what order  FlowFiles were produced</description></writesAttribute><writesAttribute><name>query.input.tables</name><description>Contains input table names in comma delimited 'databaseName.tableName' format.</description></writesAttribute></writesAttributes><stateful></stateful><restricted></restricted><inputRequirement>INPUT_ALLOWED</inputRequirement><systemResourceConsiderations></systemResourceConsiderations><seeAlso/></extension><extension><name>org.apache.nifi.processors.hive.ConvertAvroToORC</name><type>PROCESSOR</type><deprecationNotice/><description>Converts an Avro record into ORC file format. This processor provides a direct mapping of an Avro record to an ORC record, such that the resulting ORC file will have the same hierarchical structure as the Avro document. If an incoming FlowFile contains a stream of multiple Avro records, the resultant FlowFile will contain a ORC file containing all of the Avro records.  If an incoming FlowFile does not contain any records, an empty ORC file is the output. NOTE: Many Avro datatypes (collections, primitives, and unions of primitives, e.g.) can be converted to ORC, but unions of collections and other complex datatypes may not be able to be converted to ORC.</description><tags><tag>avro</tag><tag>orc</tag><tag>hive</tag><tag>convert</tag></tags><properties><property><name>orc-config-resources</name><displayName>ORC Configuration Resources</displayName><description>A file or comma separated list of files which contains the ORC configuration (hive-site.xml, e.g.). Without this, Hadoop will search the classpath for a 'hive-site.xml' file or will revert to a default configuration. Please see the ORC documentation for more details.</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>orc-stripe-size</name><displayName>Stripe Size</displayName><description>The size of the memory buffer (in bytes) for writing stripes to an ORC file</description><defaultValue>64 MB</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>orc-buffer-size</name><displayName>Buffer Size</displayName><description>The maximum size of the memory buffers (in bytes) used for compressing and storing a stripe in memory. This is a hint to the ORC writer, which may choose to use a smaller buffer size based on stripe size and number of columns for efficient stripe writing and memory utilization.</description><defaultValue>10 KB</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>orc-compression-type</name><displayName>Compression Type</displayName><description></description><defaultValue>NONE</defaultValue><allowableValues><allowableValue><displayName>NONE</displayName><value>NONE</value><description></description></allowableValue><allowableValue><displayName>ZLIB</displayName><value>ZLIB</value><description></description></allowableValue><allowableValue><displayName>SNAPPY</displayName><value>SNAPPY</value><description></description></allowableValue><allowableValue><displayName>LZO</displayName><value>LZO</value><description></description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>orc-hive-table-name</name><displayName>Hive Table Name</displayName><description>An optional table name to insert into the hive.ddl attribute. The generated DDL can be used by a PutHiveQL processor (presumably after a PutHDFS processor) to create a table backed by the converted ORC file. If this property is not provided, the full name (including namespace) of the incoming Avro record will be normalized and used as the table name.</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property></properties><dynamicProperties></dynamicProperties><relationships><relationship><name>success</name><description>A FlowFile is routed to this relationship after it has been converted to ORC format.</description><autoTerminated>false</autoTerminated></relationship><relationship><name>failure</name><description>A FlowFile is routed to this relationship if it cannot be parsed as Avro or cannot be converted to ORC for any reason</description><autoTerminated>false</autoTerminated></relationship></relationships><dynamicRelationship></dynamicRelationship><readsAttributes></readsAttributes><writesAttributes><writesAttribute><name>mime.type</name><description>Sets the mime type to application/octet-stream</description></writesAttribute><writesAttribute><name>filename</name><description>Sets the filename to the existing filename with the extension replaced by / added to by .orc</description></writesAttribute><writesAttribute><name>record.count</name><description>Sets the number of records in the ORC file.</description></writesAttribute><writesAttribute><name>hive.ddl</name><description>Creates a partial Hive DDL statement for creating a table in Hive from this ORC file. This can be used in ReplaceText for setting the content to the DDL. To make it valid DDL, add "LOCATION '&lt;path_to_orc_file_in_hdfs&gt;'", where the path is the directory that contains this ORC file on HDFS. For example, ConvertAvroToORC can send flow files to a PutHDFS processor to send the file to HDFS, then to a ReplaceText to set the content to this DDL (plus the LOCATION clause as described), then to PutHiveQL processor to create the table if it doesn't exist.</description></writesAttribute></writesAttributes><stateful></stateful><restricted></restricted><inputRequirement>INPUT_REQUIRED</inputRequirement><systemResourceConsiderations></systemResourceConsiderations><seeAlso/></extension><extension><name>org.apache.nifi.processors.hive.PutHiveStreaming</name><type>PROCESSOR</type><deprecationNotice/><description>This processor uses Hive Streaming to send flow file data to an Apache Hive table. The incoming flow file is expected to be in Avro format and the table must exist in Hive. Please see the Hive documentation for requirements on the Hive table (format, partitions, etc.). The partition values are extracted from the Avro record based on the names of the partition columns as specified in the processor. NOTE: If multiple concurrent tasks are configured for this processor, only one table can be written to at any time by a single thread. Additional tasks intending to write to the same table will wait for the current task to finish writing to the table.</description><tags><tag>hive</tag><tag>streaming</tag><tag>put</tag><tag>database</tag><tag>store</tag></tags><properties><property><name>hive-stream-metastore-uri</name><displayName>Hive Metastore URI</displayName><description>The URI location for the Hive Metastore. Note that this is not the location of the Hive Server. The default port for the Hive metastore is 9043.</description><defaultValue></defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-config-resources</name><displayName>Hive Configuration Resources</displayName><description>A file or comma separated list of files which contains the Hive configuration (hive-site.xml, e.g.). Without this, Hadoop will search the classpath for a 'hive-site.xml' file or will revert to a default configuration. Note that to enable authentication with Kerberos e.g., the appropriate properties must be set in the configuration files. Also note that if Max Concurrent Tasks is set to a number greater than one, the 'hcatalog.hive.client.cache.disabled' property will be forced to 'true' to avoid concurrency issues. Please see the Hive documentation for more details.</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-stream-database-name</name><displayName>Database Name</displayName><description>The name of the database in which to put the data.</description><defaultValue></defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-stream-table-name</name><displayName>Table Name</displayName><description>The name of the database table in which to put the data.</description><defaultValue></defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-stream-partition-cols</name><displayName>Partition Columns</displayName><description>A comma-delimited list of column names on which the table has been partitioned. The order of values in this list must correspond exactly to the order of partition columns specified during the table creation.</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-stream-autocreate-partition</name><displayName>Auto-Create Partitions</displayName><description>Flag indicating whether partitions should be automatically created</description><defaultValue>true</defaultValue><allowableValues><allowableValue><displayName>true</displayName><value>true</value><description></description></allowableValue><allowableValue><displayName>false</displayName><value>false</value><description></description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-stream-max-open-connections</name><displayName>Max Open Connections</displayName><description>The maximum number of open connections that can be allocated from this pool at the same time, or negative for no limit.</description><defaultValue>8</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-stream-heartbeat-interval</name><displayName>Heartbeat Interval</displayName><description>Indicates that a heartbeat should be sent when the specified number of seconds has elapsed. A value of 0 indicates that no heartbeat should be sent. Note that although this property supports Expression Language, it will not be evaluated against incoming FlowFile attributes.</description><defaultValue>60</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-stream-transactions-per-batch</name><displayName>Transactions per Batch</displayName><description>A hint to Hive Streaming indicating how many transactions the processor task will need. This value must be greater than 1.</description><defaultValue>100</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-stream-records-per-transaction</name><displayName>Records per Transaction</displayName><description>Number of records to process before committing the transaction. This value must be greater than 1.</description><defaultValue>10000</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-stream-call-timeout</name><displayName>Call Timeout</displayName><description>The number of seconds allowed for a Hive Streaming operation to complete. A value of 0 indicates the processor should wait indefinitely on operations. Note that although this property supports Expression Language, it will not be evaluated against incoming FlowFile attributes.</description><defaultValue>0</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>rollback-on-failure</name><displayName>Rollback On Failure</displayName><description>Specify how to handle error. By default (false), if an error occurs while processing a FlowFile, the FlowFile will be routed to 'failure' or 'retry' relationship based on error type, and processor can continue with next FlowFile. Instead, you may want to rollback currently processed FlowFiles and stop further processing immediately. In that case, you can do so by enabling this 'Rollback On Failure' property.  If enabled, failed FlowFiles will stay in the input relationship without penalizing it and being processed repeatedly until it gets processed successfully or removed by other means. It is important to set adequate 'Yield Duration' to avoid retrying too frequently.NOTE: When an error occurred after a Hive streaming transaction which is derived from the same input FlowFile is already committed, (i.e. a FlowFile contains more records than 'Records per Transaction' and a failure occurred at the 2nd transaction or later) then the succeeded records will be transferred to 'success' relationship while the original input FlowFile stays in incoming queue. Duplicated records can be created for the succeeded ones when the same FlowFile is processed again.</description><defaultValue>false</defaultValue><allowableValues><allowableValue><displayName>true</displayName><value>true</value><description></description></allowableValue><allowableValue><displayName>false</displayName><value>false</value><description></description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>kerberos-credentials-service</name><displayName>Kerberos Credentials Service</displayName><description>Specifies the Kerberos Credentials Controller Service that should be used for authenticating with Kerberos</description><defaultValue></defaultValue><controllerServiceDefinition><className>org.apache.nifi.kerberos.KerberosCredentialsService</className><groupId>org.apache.nifi</groupId><artifactId>nifi-standard-services-api-nar</artifactId><version>1.10.0</version></controllerServiceDefinition><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Kerberos Principal</name><displayName>Kerberos Principal</displayName><description>Kerberos principal to authenticate as. Requires nifi.kerberos.krb5.file to be set in your nifi.properties</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Kerberos Keytab</name><displayName>Kerberos Keytab</displayName><description>Kerberos keytab associated with the principal. Requires nifi.kerberos.krb5.file to be set in your nifi.properties</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property></properties><dynamicProperties></dynamicProperties><relationships><relationship><name>retry</name><description>The incoming FlowFile is routed to this relationship if its records cannot be transmitted to Hive. Note that some records may have been processed successfully, they will be routed (as Avro flow files) to the success relationship. The combination of the retry, success, and failure relationships indicate how many records succeeded and/or failed. This can be used to provide a retry capability since full rollback is not possible.</description><autoTerminated>false</autoTerminated></relationship><relationship><name>success</name><description>A FlowFile containing Avro records routed to this relationship after the record has been successfully transmitted to Hive.</description><autoTerminated>false</autoTerminated></relationship><relationship><name>failure</name><description>A FlowFile containing Avro records routed to this relationship if the record could not be transmitted to Hive.</description><autoTerminated>false</autoTerminated></relationship></relationships><dynamicRelationship></dynamicRelationship><readsAttributes></readsAttributes><writesAttributes><writesAttribute><name>hivestreaming.record.count</name><description>This attribute is written on the flow files routed to the 'success' and 'failure' relationships, and contains the number of records from the incoming flow file written successfully and unsuccessfully, respectively.</description></writesAttribute><writesAttribute><name>query.output.tables</name><description>This attribute is written on the flow files routed to the 'success' and 'failure' relationships, and contains the target table name in 'databaseName.tableName' format.</description></writesAttribute></writesAttributes><stateful></stateful><restricted></restricted><inputRequirement></inputRequirement><systemResourceConsiderations></systemResourceConsiderations><seeAlso/></extension><extension><name>org.apache.nifi.processors.hive.PutHiveQL</name><type>PROCESSOR</type><deprecationNotice/><description>Executes a HiveQL DDL/DML command (UPDATE, INSERT, e.g.). The content of an incoming FlowFile is expected to be the HiveQL command to execute. The HiveQL command may use the ? to escape parameters. In this case, the parameters to use must exist as FlowFile attributes with the naming convention hiveql.args.N.type and hiveql.args.N.value, where N is a positive integer. The hiveql.args.N.type is expected to be a number indicating the JDBC Type. The content of the FlowFile is expected to be in UTF-8 format.</description><tags><tag>sql</tag><tag>hive</tag><tag>put</tag><tag>database</tag><tag>update</tag><tag>insert</tag></tags><properties><property><name>Hive Database Connection Pooling Service</name><displayName>Hive Database Connection Pooling Service</displayName><description>The Hive Controller Service that is used to obtain connection(s) to the Hive database</description><defaultValue></defaultValue><controllerServiceDefinition><className>org.apache.nifi.dbcp.hive.HiveDBCPService</className><groupId>org.apache.nifi</groupId><artifactId>nifi-hive-services-api-nar</artifactId><version>1.10.0</version></controllerServiceDefinition><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-batch-size</name><displayName>Batch Size</displayName><description>The preferred number of FlowFiles to put to the database in a single transaction</description><defaultValue>100</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-charset</name><displayName>Character Set</displayName><description>Specifies the character set of the record data.</description><defaultValue>UTF-8</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>statement-delimiter</name><displayName>Statement Delimiter</displayName><description>Statement Delimiter used to separate SQL statements in a multiple statement script</description><defaultValue>;</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>rollback-on-failure</name><displayName>Rollback On Failure</displayName><description>Specify how to handle error. By default (false), if an error occurs while processing a FlowFile, the FlowFile will be routed to 'failure' or 'retry' relationship based on error type, and processor can continue with next FlowFile. Instead, you may want to rollback currently processed FlowFiles and stop further processing immediately. In that case, you can do so by enabling this 'Rollback On Failure' property.  If enabled, failed FlowFiles will stay in the input relationship without penalizing it and being processed repeatedly until it gets processed successfully or removed by other means. It is important to set adequate 'Yield Duration' to avoid retrying too frequently.</description><defaultValue>false</defaultValue><allowableValues><allowableValue><displayName>true</displayName><value>true</value><description></description></allowableValue><allowableValue><displayName>false</displayName><value>false</value><description></description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property></properties><dynamicProperties></dynamicProperties><relationships><relationship><name>retry</name><description>A FlowFile is routed to this relationship if the database cannot be updated but attempting the operation again may succeed</description><autoTerminated>false</autoTerminated></relationship><relationship><name>success</name><description>A FlowFile is routed to this relationship after the database is successfully updated</description><autoTerminated>false</autoTerminated></relationship><relationship><name>failure</name><description>A FlowFile is routed to this relationship if the database cannot be updated and retrying the operation will also fail, such as an invalid query or an integrity constraint violation</description><autoTerminated>false</autoTerminated></relationship></relationships><dynamicRelationship></dynamicRelationship><readsAttributes><readsAttribute><name>hiveql.args.N.type</name><description>Incoming FlowFiles are expected to be parametrized HiveQL statements. The type of each Parameter is specified as an integer that represents the JDBC Type of the parameter.</description></readsAttribute><readsAttribute><name>hiveql.args.N.value</name><description>Incoming FlowFiles are expected to be parametrized HiveQL statements. The value of the Parameters are specified as hiveql.args.1.value, hiveql.args.2.value, hiveql.args.3.value, and so on. The type of the hiveql.args.1.value Parameter is specified by the hiveql.args.1.type attribute.</description></readsAttribute></readsAttributes><writesAttributes><writesAttribute><name>query.input.tables</name><description>This attribute is written on the flow files routed to the 'success' relationships, and contains input table names (if any) in comma delimited 'databaseName.tableName' format.</description></writesAttribute><writesAttribute><name>query.output.tables</name><description>This attribute is written on the flow files routed to the 'success' relationships, and contains the target table names in 'databaseName.tableName' format.</description></writesAttribute></writesAttributes><stateful></stateful><restricted></restricted><inputRequirement>INPUT_REQUIRED</inputRequirement><systemResourceConsiderations></systemResourceConsiderations><seeAlso><see>org.apache.nifi.processors.hive.SelectHiveQL</see></seeAlso></extension><extension><name>org.apache.nifi.dbcp.hive.HiveConnectionPool</name><type>CONTROLLER_SERVICE</type><deprecationNotice/><description>Provides Database Connection Pooling Service for Apache Hive. Connections can be asked from pool and returned after usage.</description><tags><tag>hive</tag><tag>dbcp</tag><tag>jdbc</tag><tag>database</tag><tag>connection</tag><tag>pooling</tag><tag>store</tag></tags><properties><property><name>hive-db-connect-url</name><displayName>Database Connection URL</displayName><description>A database connection URL used to connect to a database. May contain database system name, host, port, database name and some parameters. The exact syntax of a database connection URL is specified by the Hive documentation. For example, the server principal is often included as a connection parameter when connecting to a secure Hive server.</description><defaultValue></defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-config-resources</name><displayName>Hive Configuration Resources</displayName><description>A file or comma separated list of files which contains the Hive configuration (hive-site.xml, e.g.). Without this, Hadoop will search the classpath for a 'hive-site.xml' file or will revert to a default configuration. Note that to enable authentication with Kerberos e.g., the appropriate properties must be set in the configuration files. Please see the Hive documentation for more details.</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-db-user</name><displayName>Database User</displayName><description>Database user name</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-db-password</name><displayName>Password</displayName><description>The password for the database user</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>true</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-max-wait-time</name><displayName>Max Wait Time</displayName><description>The maximum amount of time that the pool will wait (when there are no available connections)  for a connection to be returned before failing, or -1 to wait indefinitely. </description><defaultValue>500 millis</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>hive-max-total-connections</name><displayName>Max Total Connections</displayName><description>The maximum number of active connections that can be allocated from this pool at the same time, or negative for no limit.</description><defaultValue>8</defaultValue><allowableValues></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Validation-query</name><displayName>Validation query</displayName><description>Validation query used to validate connections before returning them. When a borrowed connection is invalid, it gets dropped and a new valid connection will be returned. NOTE: Using validation may have a performance penalty.</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>kerberos-credentials-service</name><displayName>Kerberos Credentials Service</displayName><description>Specifies the Kerberos Credentials Controller Service that should be used for authenticating with Kerberos</description><defaultValue></defaultValue><controllerServiceDefinition><className>org.apache.nifi.kerberos.KerberosCredentialsService</className><groupId>org.apache.nifi</groupId><artifactId>nifi-standard-services-api-nar</artifactId><version>1.10.0</version></controllerServiceDefinition><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Kerberos Principal</name><displayName>Kerberos Principal</displayName><description>Kerberos principal to authenticate as. Requires nifi.kerberos.krb5.file to be set in your nifi.properties</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Kerberos Keytab</name><displayName>Kerberos Keytab</displayName><description>Kerberos keytab associated with the principal. Requires nifi.kerberos.krb5.file to be set in your nifi.properties</description><defaultValue></defaultValue><allowableValues></allowableValues><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>VARIABLE_REGISTRY</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property></properties><dynamicProperties></dynamicProperties><stateful></stateful><restricted></restricted><inputRequirement></inputRequirement><systemResourceConsiderations></systemResourceConsiderations><seeAlso/><providedServiceAPIs><providedServiceAPI><className>org.apache.nifi.dbcp.hive.HiveDBCPService</className><groupId>org.apache.nifi</groupId><artifactId>nifi-hive-services-api-nar</artifactId><version>1.10.0</version></providedServiceAPI></providedServiceAPIs></extension></extensions></extensionManifest>